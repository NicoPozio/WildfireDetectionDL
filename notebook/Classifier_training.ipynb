{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPuU/lXSmxsfbTFjU0WO0bM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicoPozio/WildfireDetectionDL/blob/main/notebook/Classifier_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NicoPozio/WildfireDetectionDL.git"
      ],
      "metadata": {
        "id": "NUhUF7tDYgzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "repo_path = '/content/WildfireDetectionDL'\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.append(repo_path)"
      ],
      "metadata": {
        "id": "F1sXhKEnf1dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Update Code\n",
        "import os\n",
        "\n",
        "REPO_PATH = \"/content/WildfireDetectionDL\"\n",
        "\n",
        "print(\"ðŸ”„ Syncing with GitHub...\")\n",
        "!cd {REPO_PATH} && git pull\n",
        "\n",
        "print(\"Code updated. You can now run the Sweep or Train cell immediately.\")"
      ],
      "metadata": {
        "id": "hpvQdBgJvSDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Project Dependencies\n",
        "#hydra-core: Configuration management\n",
        "#wandb: Experiment tracking\n",
        "#omegaconf: Dict handling for Hydra\n",
        "!pip install -q hydra-core wandb omegaconf\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive Mounted\")\n"
      ],
      "metadata": {
        "id": "xYMO3qvxQo_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "try:\n",
        "    #Fetch key from Colab Secrets\n",
        "    api_key = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "    #Set as Environment Variable\n",
        "    #This ensures Hydra and subprocesses can find it automatically\n",
        "    os.environ[\"WANDB_API_KEY\"] = api_key\n",
        "\n",
        "    wandb.login()\n",
        "    print(\"Logged in to WandB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Authentication Failed: {e}\")\n",
        "    print(\"Action Required: Go to the 'Secrets' tab (Key icon) on the left.\")\n",
        "    print(\"Add a new secret named 'WANDB_API_KEY' with your key from https://wandb.ai/authorize\")"
      ],
      "metadata": {
        "id": "HRabcBKtQqgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we download the dataset, we check if the user has the data in its google drive, if not it's downloaded from a public link"
      ],
      "metadata": {
        "id": "Ti3-Olu9Xnqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "509SQeSJXm1k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "from src.utils import extract_zip\n",
        "\n",
        "\n",
        "\n",
        "#destination\n",
        "local_root = \"/content/data\"\n",
        "os.makedirs(local_root, exist_ok=True)\n",
        "!rm -rf {local_root}\n",
        "\n",
        "#Check if files exist in the mounted drive\n",
        "#If the users executed the cycle_gan notebook he will have the dataset saved on his drive\n",
        "drive_dataset_path = \"/content/drive/MyDrive/Wildfire_Project/dataset.zip\"\n",
        "drive_synthetic_path = \"/content/drive/MyDrive/Wildfire_Project/synthetic_wildfire_2k.zip\"\n",
        "\n",
        "#If not we can download it from this public (non-modifiable) drive folder\n",
        "public_drive_folder_url = \"https://drive.google.com/drive/folders/1bH_5LoY0jJ7NQj1fmhpxpi4T7UvWa9gG?usp=drive_link\"\n",
        "\n",
        "\n",
        "#Check if files exist in the mounted drive\n",
        "if os.path.isfile(drive_dataset_path) and os.path.isfile(drive_synthetic_path):\n",
        "    # Assign paths to the Drive locations\n",
        "    target_dataset_zip = drive_dataset_path\n",
        "    target_synthetic_zip = drive_synthetic_path\n",
        "\n",
        "else:\n",
        "\n",
        "    # Download the entire folder\n",
        "    # --quiet=False ensures the download progress bar is hidden\n",
        "    gdown.download_folder(url=public_drive_folder_url, output=local_root, quiet=False)\n",
        "\n",
        "    # assign paths to the locally downloaded versions\n",
        "    target_dataset_zip = os.path.join(local_root, \"dataset.zip\")\n",
        "    target_synthetic_zip = os.path.join(local_root, \"synthetic_wildfire_2k.zip\")\n",
        "\n",
        "    # Validation: Ensure download actually resulted in the required files\n",
        "    if not (os.path.exists(target_dataset_zip) and os.path.exists(target_synthetic_zip)):\n",
        "        raise FileNotFoundError(f\"Download completed, but expected zip files were not found in {local_root}. Check the public Drive folder contents.\")\n",
        "\n",
        "print(f\"\\nBeginning extraction\")\n",
        "\n",
        "# Extract Dataset\n",
        "extract_zip(target_dataset_zip, local_root)\n",
        "\n",
        "# Extract Synthetic Dataset\n",
        "extract_zip(target_synthetic_zip, local_root)\n",
        "\n",
        "print(\"\\nDATA PREPARATION COMPLETE.\")\n",
        "print(f\"Extracted data is available in: {local_root}\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define your data root\n",
        "DATA_ROOT = \"/content/data\"\n",
        "\n",
        "print(f\"Scanning {DATA_ROOT} for corrupt images...\")\n",
        "\n",
        "corrupt_count = 0\n",
        "for root, dirs, files in os.walk(DATA_ROOT):\n",
        "    for filename in files:\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            file_path = os.path.join(root, filename)\n",
        "            try:\n",
        "                # Try to fully load the image bytes\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.load()\n",
        "            except OSError:\n",
        "                print(f\"Found corrupt image: {file_path}\")\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    print(f\"Deleted {filename}\")\n",
        "                    corrupt_count += 1\n",
        "                except:\n",
        "                    print(f\"Could not delete {filename}\")\n",
        "\n",
        "print(f\"\\nScan Complete. Removed {corrupt_count} corrupt files.\")"
      ],
      "metadata": {
        "id": "pNVIFW9gqYb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# 1. Define Paths Constants\n",
        "REPO_ROOT = \"/content/WildfireDetectionDL\"\n",
        "SWEEP_CONFIG_PATH = os.path.join(REPO_ROOT, \"conf\", \"sweep.yaml\")\n",
        "\n",
        "# 2. Validation\n",
        "if not os.path.exists(SWEEP_CONFIG_PATH):\n",
        "    print(f\"CRITICAL ERROR: Could not find sweep.yaml at {SWEEP_CONFIG_PATH}\")\n",
        "else:\n",
        "    print(f\"--- Registering Sweep from {SWEEP_CONFIG_PATH} ---\")\n",
        "\n",
        "    # 3. Register the sweep\n",
        "    result = subprocess.run(\n",
        "        [\"wandb\", \"sweep\", SWEEP_CONFIG_PATH],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        # CORRECTION: cwd must be the directory, not the file\n",
        "        cwd=REPO_ROOT\n",
        "    )\n",
        "\n",
        "    output_text = result.stderr + result.stdout\n",
        "    print(\"Raw Output:\", output_text)\n",
        "\n",
        "    # 4. Extract Sweep ID\n",
        "    sweep_id = None\n",
        "    for line in output_text.split('\\n'):\n",
        "        if \"wandb agent\" in line:\n",
        "            parts = line.strip().split(\"wandb agent \")\n",
        "            if len(parts) > 1:\n",
        "                sweep_id = parts[-1].strip()\n",
        "                break\n",
        "\n",
        "    # 5. Launch the Agent\n",
        "    if sweep_id:\n",
        "        print(f\"\\nSUCCESS: Detected Sweep ID: {sweep_id}\")\n",
        "        print(\"Starting Agent... (This will run multiple experiments)\")\n",
        "\n",
        "        # CORRECTION: We use the explicit REPO_ROOT variable defined at the top\n",
        "        !cd {REPO_ROOT} && wandb agent {sweep_id} --count 20\n",
        "    else:\n",
        "        print(\"\\nERROR: Could not find Sweep ID.\")"
      ],
      "metadata": {
        "id": "OE8v94eqa7zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Configuration\n",
        "# ---------------------------------------------------------\n",
        "# Based on your logs: https://wandb.ai/pozioniccolo-sapienza-universit-di-roma/WildfireDetectionDL/sweeps/7hmucehp\n",
        "\n",
        "# REPLACE THESE WITH YOUR EXACT DETAILS FROM THE LOGS\n",
        "ENTITY = \"pozioniccolo-sapienza-universit-di-roma\"\n",
        "PROJECT = \"WildfireDetectionDL\"  # <--- Correction: Case Sensitive!\n",
        "SWEEP_ID = \"7hmucehp\"            # <--- Correction: Hardcoded from your logs to be safe\n",
        "\n",
        "# Construct the full path\n",
        "sweep_path = f\"{ENTITY}/{PROJECT}/{SWEEP_ID}\"\n",
        "print(f\"Connecting to: {sweep_path}...\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Fetch Data\n",
        "# ---------------------------------------------------------\n",
        "api = wandb.Api()\n",
        "\n",
        "try:\n",
        "    sweep = api.sweep(sweep_path)\n",
        "    print(\"âœ… Connection Successful! Downloading runs...\")\n",
        "\n",
        "    # Get runs sorted by accuracy\n",
        "    runs = sorted(sweep.runs, key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\n",
        "\n",
        "    results = []\n",
        "    for run in runs:\n",
        "        # Only process runs that finished and have metrics\n",
        "        if \"val_acc\" in run.summary:\n",
        "            cfg = run.config\n",
        "\n",
        "            # Helper to safely get nested keys\n",
        "            def get_param(config_dict, key_path):\n",
        "                keys = key_path.split('.')\n",
        "                val = config_dict\n",
        "                for k in keys:\n",
        "                    if isinstance(val, dict):\n",
        "                        val = val.get(k)\n",
        "                    else:\n",
        "                        return None\n",
        "                return val\n",
        "\n",
        "            # Extract data using the safe helper\n",
        "            results.append({\n",
        "                \"run_id\": run.id,\n",
        "                \"model\": get_param(cfg, 'model.name'),\n",
        "                \"use_synthetic\": get_param(cfg, 'dataset.params.use_synthetic'),\n",
        "                \"lr\": get_param(cfg, 'training.learning_rate'),\n",
        "                \"batch_size\": get_param(cfg, 'training.batch_size'),\n",
        "                \"optimizer\": get_param(cfg, 'training.optimizer'),\n",
        "                \"val_acc\": run.summary.get(\"val_acc\")\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. Analysis & Visualization\n",
        "    # ---------------------------------------------------------\n",
        "    if not df.empty:\n",
        "        best_run = df.iloc[0]\n",
        "        print(\"\\nðŸ† CHAMPION CONFIGURATION ðŸ†\")\n",
        "        print(f\"Run ID:         {best_run['run_id']}\")\n",
        "        print(f\"Model:          {best_run['model']}\")\n",
        "        print(f\"Synthetic Data: {best_run['use_synthetic']}\")\n",
        "        print(f\"Optimizer:      {best_run['optimizer']}\")\n",
        "        print(f\"Learning Rate:  {best_run['lr']}\")\n",
        "        print(f\"Val Accuracy:   {best_run['val_acc']:.2f}%\")\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=df, x=\"model\", y=\"val_acc\", hue=\"use_synthetic\", errorbar=None)\n",
        "        plt.title(\"Model Performance: Real vs Synthetic Data\")\n",
        "        plt.ylim(80, 100) # Zoom in to the top range (since your acc is high)\n",
        "        plt.ylabel(\"Validation Accuracy (%)\")\n",
        "        plt.xlabel(\"Model Architecture\")\n",
        "        plt.legend(title=\"Use Synthetic Data\")\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"âš ï¸ No finished runs found with validation accuracy.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error connecting to WandB: {e}\")\n",
        "    print(\"Check if Entity, Project, or Sweep ID are correct.\")"
      ],
      "metadata": {
        "id": "R4oXx9N0c73U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "# 1. Configuration (Must match your logs exactly)\n",
        "ENTITY = \"pozioniccolo-sapienza-universit-di-roma\"\n",
        "PROJECT = \"WildfireDetectionDL\"\n",
        "\n",
        "api = wandb.Api()\n",
        "\n",
        "if 'best_run' in locals() and not df.empty:\n",
        "    # Get the ID from the pandas row\n",
        "    run_id = best_run['run_id']\n",
        "\n",
        "    print(f\"ðŸ† Preparing to test Best Model: {best_run['model']} (Run ID: {run_id})...\")\n",
        "\n",
        "    # 2. Construct the path using known variables\n",
        "    # Format: entity/project/run_id\n",
        "    run_path = f\"{ENTITY}/{PROJECT}/{run_id}\"\n",
        "\n",
        "    print(f\"â¬‡ï¸ Downloading weights from: {run_path}\")\n",
        "\n",
        "    try:\n",
        "        # 3. Download 'best_model.pth'\n",
        "        run_obj = api.run(run_path)\n",
        "        run_obj.file(\"best_model.pth\").download(replace=True, root=\".\")\n",
        "\n",
        "        print(\"âœ… Download Complete. Weights are ready.\")\n",
        "\n",
        "        # 4. Run the Test\n",
        "        print(\"ðŸš€ Starting Test Script...\")\n",
        "\n",
        "        # Extract params for the command line\n",
        "        use_syn = str(best_run['use_synthetic']).lower()\n",
        "        batch_size = int(best_run['batch_size'])\n",
        "\n",
        "        !python ./WildfireDetectionDL/test.py \\\n",
        "          model.name={best_run['model']} \\\n",
        "          dataset.params.use_synthetic={use_syn} \\\n",
        "          training.batch_size={batch_size} \\\n",
        "          wandb.mode=\"online\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"Cannot run test: No best run identified in the analysis step.\")"
      ],
      "metadata": {
        "id": "AujNSNr9Vfy1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}